{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[price]|[discount]' '[price]|$MONEY' '[discount]|[price]'\n",
      " '[time]|[price]' '$MONEY|[price]']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"30.csv\")\n",
    "inputs = df.iloc[:,3:].values\n",
    "outs = df[\"labels\"].values\n",
    "\n",
    "\n",
    "\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "X_new = selector.fit_transform(inputs, outs)\n",
    "cols = selector.get_support(indices=True)\n",
    "selected_patterns = np.take(df.columns.values,[x+3 for x in cols] )\n",
    "print(selected_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'labels', 'sentences', 'PROPN', 'PROPN+NOUN', 'PROPN+*+NOUN',\n",
       "       'PROPN+*+[deal]', 'PROPN+*+[deal]|NOUN', 'PROPN+[deal]', 'PROPN|NUM',\n",
       "       ...\n",
       "       '$MONEY|[price]+*+PRON+VERB', '$MONEY|[price]+*+PRON', '$MONEY|[deal]',\n",
       "       '$MONEY|[discount]', '$MONEY|[time]', '$MONEY|[time]+NOUN',\n",
       "       '$MONEY|[time]+*+NOUN', '$MONEY|$ORDINAL', '$MONEY|$ORDINAL+NOUN',\n",
       "       '$MONEY|$ORDINAL+*+NOUN'],\n",
       "      dtype='object', length=151)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_new = df.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['labels', 'PROPN', 'PROPN+NOUN', 'PROPN+*+NOUN', 'PROPN+*+[deal]',\n",
       "       'PROPN+*+[deal]|NOUN', 'PROPN+[deal]', 'PROPN|NUM', 'PROPN|NUM+NOUN',\n",
       "       'PROPN|NUM+*+NOUN',\n",
       "       ...\n",
       "       '$MONEY|[price]+*+PRON+VERB', '$MONEY|[price]+*+PRON', '$MONEY|[deal]',\n",
       "       '$MONEY|[discount]', '$MONEY|[time]', '$MONEY|[time]+NOUN',\n",
       "       '$MONEY|[time]+*+NOUN', '$MONEY|$ORDINAL', '$MONEY|$ORDINAL+NOUN',\n",
       "       '$MONEY|$ORDINAL+*+NOUN'],\n",
       "      dtype='object', length=149)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = df_new.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "reduced_matrix = matrix.mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PROPN+NOUN</th>\n",
       "      <th>PROPN+*+NOUN</th>\n",
       "      <th>PROPN+*+[deal]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.067267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN+NOUN</th>\n",
       "      <td>0.212351</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN+*+NOUN</th>\n",
       "      <td>0.156957</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN+*+[deal]</th>\n",
       "      <td>0.212351</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  labels     PROPN  PROPN+NOUN  PROPN+*+NOUN  PROPN+*+[deal]\n",
       "labels               NaN       NaN         NaN           NaN             NaN\n",
       "PROPN           0.067267       NaN         NaN           NaN             NaN\n",
       "PROPN+NOUN      0.212351  0.371391         NaN           NaN             NaN\n",
       "PROPN+*+NOUN    0.156957  0.666667    0.557086           NaN             NaN\n",
       "PROPN+*+[deal]  0.212351  0.371391    1.000000      0.557086             NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_matrix.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [c for c in reduced_matrix.columns if any(reduced_matrix[c] ==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN+NOUN',\n",
       " 'PROPN+*+NOUN',\n",
       " 'PROPN+*+[deal]',\n",
       " 'PROPN+*+[deal]|NOUN',\n",
       " 'PROPN+[deal]',\n",
       " 'PROPN|NUM',\n",
       " 'PROPN|NUM+NOUN',\n",
       " 'PROPN|NUM+*+NOUN',\n",
       " 'PROPN|[price]',\n",
       " 'PROPN|[deal]',\n",
       " 'PROPN|[discount]',\n",
       " 'PROPN|[time]',\n",
       " 'PROPN|[time]+NOUN',\n",
       " 'PROPN|[time]+*+NOUN',\n",
       " 'PROPN|[time]+*+NOUN+ADV',\n",
       " 'PROPN|$MONEY',\n",
       " 'PROPN|$MONEY+NOUN',\n",
       " 'PROPN|$MONEY+*+NOUN',\n",
       " 'PROPN|$ORDINAL+NOUN',\n",
       " 'NUM',\n",
       " 'NUM+NOUN',\n",
       " 'NUM+*+PRON',\n",
       " 'NUM+*+PRON|PROPN',\n",
       " 'NUM+*+PRON|NOUN',\n",
       " 'NUM+*+PRON|$ORG',\n",
       " 'NUM+*+PROPN|PRON',\n",
       " 'NUM+*+PROPN',\n",
       " 'NUM+*+NOUN',\n",
       " 'NUM+*+$ORG|PRON',\n",
       " 'NUM+*+$ORG',\n",
       " 'NUM|[price]',\n",
       " 'NUM|[price]+*+PRON+VERB',\n",
       " 'NUM|[price]+*+PRON',\n",
       " 'NUM|[price]+*+PROPN|PRON',\n",
       " 'NUM|[price]+*+PROPN',\n",
       " 'NUM|[deal]',\n",
       " 'NUM|[discount]',\n",
       " 'NUM|[time]',\n",
       " 'NUM|[time]+NOUN',\n",
       " 'NUM|[time]+*+NOUN',\n",
       " 'NUM|$ORDINAL+NOUN',\n",
       " 'NUM|$ORDINAL+*+NOUN',\n",
       " '[price]+VERB|ADV',\n",
       " '[price]+VERB|AUX',\n",
       " '[price]+ADV|AUX',\n",
       " '[price]+AUX|ADV',\n",
       " '[price]|NUM+*+PRON+VERB',\n",
       " '[price]|NUM+*+PRON',\n",
       " '[price]|[deal]',\n",
       " '[price]|[deal]+*+VERB',\n",
       " '[price]|[deal]+*+VERB|ADV',\n",
       " '[price]|[deal]+*+ADV',\n",
       " '[price]|[deal]+*+ADV|VERB',\n",
       " '[price]|[discount]',\n",
       " '[price]|[discount]+*+ADJ',\n",
       " '[price]|[time]',\n",
       " '[price]|[time]+ADV',\n",
       " '[price]|[time]+*+ADV',\n",
       " '[price]|$MONEY',\n",
       " '[price]|$MONEY+*+PRON+VERB',\n",
       " '[price]|$MONEY+*+PRON',\n",
       " '[price]|$ORDINAL+*+ADJ',\n",
       " '[deal]|[price]+*+VERB|ADV',\n",
       " '[deal]|[discount]',\n",
       " '[deal]|[time]',\n",
       " '[deal]|[time]+*+ADV',\n",
       " '[deal]|$MONEY',\n",
       " '[discount]|[time]',\n",
       " '[discount]|$MONEY',\n",
       " '[time]',\n",
       " '[time]|PROPN+NOUN',\n",
       " '[time]|[price]+ADV',\n",
       " '[time]|$MONEY',\n",
       " '[time]|$MONEY+NOUN',\n",
       " '[time]|$MONEY+*+NOUN',\n",
       " '[time]|$ORG',\n",
       " '[time]|$ORG+NOUN',\n",
       " '$MONEY',\n",
       " '$MONEY+NOUN',\n",
       " '$MONEY+*+PRON+VERB',\n",
       " '$MONEY|[price]+*+PRON+VERB',\n",
       " '$MONEY|[time]+NOUN',\n",
       " '$MONEY|$ORDINAL+NOUN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df_new.drop(to_drop, axis=1)\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconstructed = df[df_reduced.columns.values]\n",
    "df_reconstructed = df_reconstructed.drop(\"labels\", axis=1)\n",
    "# df_reconstructed[\"id\"] = df['id']\n",
    "# df_reconstructed[\"sentences\"] = df[\"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[deal]|[price]' '[discount]|[price]' '[discount]|[deal]'\n",
      " '[time]|[price]' '$MONEY|[price]']\n"
     ]
    }
   ],
   "source": [
    "inputs = df_reconstructed.values\n",
    "outs = df[\"labels\"].values\n",
    "\n",
    "\n",
    "\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "X_new = selector.fit_transform(inputs, outs)\n",
    "cols = selector.get_support(indices=True)\n",
    "selected_patterns = np.take(df_reconstructed.columns.values,[x for x in cols] )\n",
    "print(selected_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "# Create a custom diverging palette\n",
    "cmap = sns.diverging_palette(250, 15, s=75, l=40,\n",
    "                             n=9, center=\"light\", as_cmap=True)\n",
    "\n",
    "_ = sns.heatmap(matrix, center=0, annot=True, \n",
    "                fmt='.2f', square=True, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "collector = {}\n",
    "labels = df['labels']\n",
    "all_cols = df.columns.values[4:]\n",
    "for col in all_cols:\n",
    "    col_selected = df[col]\n",
    "    fscore = precision_recall_fscore_support(labels, col_selected,  average=\"binary\")[2]\n",
    "    collector[col] =  fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_selected = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = {k: v for k, v in sorted(collector.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = list(collector.keys())[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_selected.append(starter)\n",
    "to_drop = [c for c in corr.columns if corr[starter][c] >= 0.8]\n",
    "df_reduced = df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26584/1975192939.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new[coll] = np.logical_or(df_reduced[coll], df[starter])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_new = pd.DataFrame()\n",
    "new_cols = df_reduced.columns.values[4:]\n",
    "for coll in new_cols:\n",
    "    \n",
    "    df_new[coll] = np.logical_or(df_reduced[coll], df[starter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 101)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_collector = {}\n",
    "for col in new_cols:\n",
    "    col_selected = df_reduced[col]\n",
    "    fscore = precision_recall_fscore_support(labels, col_selected,  average=\"binary\")[2]\n",
    "    new_collector[col] =  fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('$PERCENT|[pay]', ['$PERCENT|[pay]'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starter, patterns_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_collector = {k: v for k, v in sorted(new_collector.items(), key=lambda item: item[1])}\n",
    "new_collector\n",
    "next_starter = list(new_collector.keys())[-1]\n",
    "patterns_selected.append(next_starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[expensive]|[pay]+*+PRON'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_reduced.corr()\n",
    "to_drop = [c for c in corr.columns if corr[next_starter][c] >= 0.8]\n",
    "df_reduced = df_reduced.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[pay]|[expensive]',\n",
       " '[pay]|[expensive]+*+PRON',\n",
       " '[pay]|[expensive]+*+PRON+AUX',\n",
       " '[expensive]|[pay]',\n",
       " '[expensive]|[pay]+*+PRON',\n",
       " '[expensive]|[pay]+*+PRON+AUX']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def train_and_report(patterns, inputs, outputs):\n",
    "    #Change numpy inputs to tensors \n",
    "    outputs = torch.tensor(outputs).reshape(-1,1)\n",
    "    inputs = torch.tensor(inputs)\n",
    "\n",
    "    #train the linear layer for 100 iterations\n",
    "    #100 chosen at random TODO see what a good number is for iteration\n",
    "\n",
    "    net = torch.nn.Linear(inputs.shape[1],1, bias=False)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for e in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        o =  sigmoid.forward(net.forward(inputs.float()))\n",
    "            \n",
    "        loss = criterion(o, outputs.float())\n",
    "            \n",
    "        losses.append(loss.sum().item())\n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "    \n",
    "    pred =  sigmoid.forward(net.forward(inputs.float())).detach().numpy()>0.5\n",
    "    labeled_prf = precision_recall_fscore_support(outputs, pred, average=\"weighted\")\n",
    "\n",
    "    fscore = labeled_prf[2]\n",
    "    # print(f\"{patterns}, {fscore}\")\n",
    "\n",
    "\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# define df, columns, true labels\n",
    "def feature_selector(df):\n",
    "\n",
    "    print(f\"==================================Start of Feature Selection===========================================\")\n",
    "    labels = df['labels']\n",
    "    jj = 0\n",
    "    ### Controller variables\n",
    "    patterns_selected = []\n",
    "    highest_fscore = \"0.0\"\n",
    "    df_subset = pd.DataFrame()\n",
    "    remaining_cols = df.columns.values[4:]\n",
    "\n",
    "\n",
    "\n",
    "    outputs = df[\"labels\"].values\n",
    "    while len(patterns_selected)<10 and len(remaining_cols)>0:\n",
    "        jj += 1\n",
    "        print(f\"Starting iteration {jj} {len(remaining_cols)}\")\n",
    "        #first calculate the fscore\n",
    "        collector = {}\n",
    "        local_max_fscore = \"0.0\"\n",
    "        for col in remaining_cols:\n",
    "            col_selected = df[col].astype('int64')\n",
    "            current_patterns = patterns_selected+[col]\n",
    "            current_df = pd.concat([df_subset, col_selected], axis=1)\n",
    "            inputs = current_df.values\n",
    "            \n",
    "            # fscore = precision_recall_fscore_support(labels, col_selected,  average=\"binary\")[2]\n",
    "            fscore = train_and_report(current_patterns, inputs, outputs)\n",
    "            \n",
    "                \n",
    "            exists = str(fscore) in collector\n",
    "            if(exists):\n",
    "                collector[str(fscore)].append(col)\n",
    "                \n",
    "            else:\n",
    "                collector[str(fscore)] = [col]\n",
    "        #sort and get a pattern with high fscore\n",
    "        selected_starter_pattern = list(collector.values())[-1]\n",
    "        collector = {k: v for k, v in sorted(collector.items(), key=lambda item: item[0])}\n",
    "        current_fscore = list(collector.keys())[-1]\n",
    "\n",
    "        if(current_fscore>highest_fscore):\n",
    "            highest_fscore = current_fscore\n",
    "        else:\n",
    "            # print(f\"{highest_fscore} {current_fscore}, {selected_starter_pattern}\")\n",
    "            break\n",
    "        selected_starter_pattern = list(collector.values())[-1]\n",
    "        # print(selected_starter_pattern)\n",
    "\n",
    "        #Group the correlated ones and pick the shortest\n",
    "        rowss = df[selected_starter_pattern]\n",
    "        correlation = rowss.corr()\n",
    "        correlation.loc[:,:] =  np.tril(correlation, k=-1)\n",
    "        cor = correlation.stack()\n",
    "        ones = cor[cor >=0.8].reset_index().loc[:,['level_0','level_1']]\n",
    "        ones = ones.query('level_0 not in level_1')\n",
    "        grps = list(ones.groupby('level_0').groups.keys())\n",
    "        colls = []\n",
    "        for i in grps:\n",
    "            groups = ones[ones[\"level_0\"]==i].values\n",
    "            set_maker = []\n",
    "            for patterns in groups:\n",
    "                set_maker += patterns.tolist()\n",
    "            colls.append(sorted(set_maker, key=len)[0])\n",
    "            \n",
    "        for selected_starter_pattern in colls:\n",
    "            patterns_selected.append(selected_starter_pattern)\n",
    "            df_subset[selected_starter_pattern] = df[selected_starter_pattern].astype('int64')\n",
    "            try:\n",
    "                selected_starter_series = df[selected_starter_pattern][0]\n",
    "                \n",
    "                corr = df.corr()\n",
    "                to_drop = [c for c in corr.columns if corr[selected_starter_pattern][c] >= 0.8] #0.8 chosen at random\n",
    "                df = df.drop(to_drop, axis=1)\n",
    "\n",
    "                #create a new df with combination of current one\n",
    "                remaining_cols = df.columns.values[4:]\n",
    "                for collumn in remaining_cols:\n",
    "                    df[collumn] = np.logical_or(df[collumn], selected_starter_series)\n",
    "            except:\n",
    "                print(\"We already removed \", selected_starter_pattern)\n",
    "            for coll in remaining_cols:\n",
    "                df[coll] = np.logical_or(df[coll], selected_starter_series)\n",
    "        \n",
    "        print(f\"Finishing iteration {jj} {len(remaining_cols)}, --- {patterns_selected}, {highest_fscore}\")\n",
    "    \n",
    "    print(f\"---------------------------Summary---------------------------\")\n",
    "    print(f\"Patterns {patterns_selected}\")\n",
    "    print(f\"Positive examples \\n{df[df['labels']==1]['sentences'].values}\")\n",
    "    print(f\"Negative examples \\n{df[df['labels']==0]['sentences'].values}\")\n",
    "\n",
    "    print(f\"==================================End of Feature Selection===========================================\")\n",
    "    return patterns_selected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 151)\n",
      "==================================Start of Feature Selection===========================================\n",
      "Starting iteration 1 147\n",
      "Finishing iteration 1 143, --- ['$MONEY|[price]', '[discount]|[price]'], 0.8241758241758242\n",
      "Starting iteration 2 143\n",
      "---------------------------Summary---------------------------\n",
      "Patterns ['$MONEY|[price]', '[discount]|[price]']\n",
      "Positive examples \n",
      "['Prices vary so you have to shop around, but every once in a while you can find a really awesome deal.\\n'\n",
      " \"For the amenities and $ paid, it's small.\\n\"\n",
      " 'Me : Then why did you tell me they were on discount ?\\n'\n",
      " 'Adult tickets are $ 13.50 but they have military discount .'\n",
      " 'When we were about to pay , the server handed us a bill and I reminded her that we were using the 1/2 off Scoutmob deal , so she took the bill back and came back with another that was pretty much the same except $ 8 difference ... REALLY ? ! ?'\n",
      " 'You get a much much larger portion for the same price at SFU (the bowl is over-flowing).\\n'\n",
      " 'Prices were reasonable and mission accomplished!\\n'\n",
      " 'Large selection of dishes for under $9 and there is tons of food.'\n",
      " 'They have coupons discount for first second and third time customers through their website .\\n'\n",
      " 'My other dining companions seemed pleased with their food ( one ordered a veal marsala that I did not try and the other - my favorite - ordered off the menu , actually , a request they were kind enough to accommodate - it was gnocchi caprese with pesto sauce instead of tomato sauce , and it was gooeyyummydelicious ) !\\n'\n",
      " 'The shoe was on special price so we got a good deal and bought the shoe as well!\\n'\n",
      " \"A little expensive but I'll definitely be back time and time again\\n\"\n",
      " 'Prices here are very reasonable!\\n']\n",
      "Negative examples \n",
      "['needless to say whomever told me this , is a genius .\\n'\n",
      " 'the waiter was friendly and helpful\\n'\n",
      " 'All the instructors are friendly and helpful. \\n'\n",
      " \"The staff is always friendly and they make you feel comfortable even when you're getting uncomfortable work done.\\n\"\n",
      " 'I did end up finding the perfect doll.\\n'\n",
      " 'Only downside : three entrees served simultaneously but fourth ( beet/ goat cheese ravioli ) was 2 - 3 minutes later which allowed my accompaniments ( delicious creamy polenta ) to cool off to much .'\n",
      " 'The store was clean and well organized\\n'\n",
      " 'Well, massively disappointed with this place. Fries were cold, I mean 5 HOUR cold!! The burger? Smashed together like they were trying to save space in bvb yge bag.'\n",
      " 'This man is unstable and incompetent as a manager\\n'\n",
      " 'The place was beautiful, immaculately clean, comfortable bedding (lovely sheets) and breakfast was delicious.\\n'\n",
      " 'Super quick food lines and attentive employees.\\n'\n",
      " 'Parking can be a issue and they no longer have the dog pen.\\n'\n",
      " \"We arrived at 6 pm and the employees told us the order was n't ready .\"\n",
      " 'Do not eat here, it is unsanitary.\\n'\n",
      " \"It's such a modern restaurant but their dishes are authentic and super delicious\\n\"\n",
      " 'Our server actually seemed annoyed that he had to talk with us... Just another table and more faces... More orders... Move it along. \\n'\n",
      " 'Very very friendly and they made an effort with us.']\n",
      "==================================End of Feature Selection===========================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"30.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "features = feature_selector(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m#get rid of all features correlated \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=12'>13</a>\u001b[0m corr \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcorr()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=14'>15</a>\u001b[0m to_drop \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m corr\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m corr[selected_patterns][c] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m] \u001b[39m#0.8 chosen at random\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=17'>18</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(to_drop, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb Cell 38'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m#get rid of all features correlated \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=12'>13</a>\u001b[0m corr \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcorr()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=14'>15</a>\u001b[0m to_drop \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m corr\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m corr[selected_patterns][c] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m] \u001b[39m#0.8 chosen at random\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsimret-instance-1/home/simret/trial/pattern_synthesizer_api/temp/feature_sel.ipynb#ch0000039vscode-remote?line=17'>18</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(to_drop, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/simret/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"30.csv\")\n",
    "inputs = df.iloc[:,3:].values\n",
    "outs = df[\"labels\"].values\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "\n",
    "    selector = SelectKBest(f_classif, k=1)\n",
    "    X_new = selector.fit_transform(inputs, outs)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    selected_patterns = np.take(df.columns.values,[x+3 for x in cols] )\n",
    "    #get rid of all features correlated \n",
    "    corr = df.corr()\n",
    "    \n",
    "    to_drop = [c for c in corr.columns if corr[selected_patterns][c] >= 0.8] #0.8 chosen at random\n",
    "    print(df.shape)\n",
    "\n",
    "    # df = df.drop(to_drop, axis=1)\n",
    "    print(len(to_drop))\n",
    "    \n",
    "    print(df.shape)\n",
    "\n",
    "    \n",
    "    print(selected_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[price]|[discount][price]|$MONEY[discount]|[price][time]|[price]$MONEY|[price]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['[price]|[discount]' '[price]|$MONEY' '[discount]|[price]'\n",
    " '[time]|[price]' '$MONEY|[price]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25bcad2c992ed4a2be145129017787896f246deb7396e6b823d462a0062482c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
